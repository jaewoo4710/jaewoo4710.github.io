---
title:  "Bayesian Estimation 따라하기"
excerpt: "Blog"

toc: true
toc_sticky: true

categories:
  - Blog
tags:
  - Bayesian
last_modified_at: 2020-09-15T08:06:00-05:00
---



# Bayesian Estimation 
[이 글](https://towardsdatascience.com/hands-on-bayesian-statistics-with-python-pymc3-arviz-499db9a59501)에서는 간단히 Python에서 Pym 3 와 Arvie를 통해서 Bayesian estimation을 설명하는 글이다. Spanish High Speed Rail tickets pricing data set 통하여 기찻값의 모 분포를 예측 하는 것이 목표이다.
# 데이터 세트
```python
from scipy import stats
import arviz as az
import numpy as np
import matplotlib.pyplot as plt
import pymc3 as pm
import seaborn as sns
import pandas as pd
from theano import shared
from sklearn import preprocessing
print('Running on PyMC3 v{}'.format(pm.__version__))
data = pd.read_csv('https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/data/renfe_small.csv')
data.head(2)
```
데이터 세트를 불러올 때, `head`를 통해서 header와 데이터값이 잘 들어가 있는지 확인한다. 이 과정을 습관화하자.
다음으로는 바로 NaN 값의 확인이다.

```py
data.isnull().sum()/len(data)
```
> insert_date    0.000000
origin         0.000000
destination    0.000000
start_date     0.000000
end_date       0.000000
train_type     0.000000
price          0.119467
train_class    0.003993
fare           0.003993
dtype: float64

여기서 우리가 주목해봐야 할 것은 어떻게 이 NaN의 값을 처리하였는가이다.

```py
data['train_class'] = data['train_class'].fillna(data['train_class'].mode().iloc[0])
data['fare'] = data['fare'].fillna(data['fare'].mode().iloc[0])
data['price'] = data.groupby('fare').transform(lambda x: x.fillna(x.mean()))
```
**train_class** 열차의 등급을 나타내고, 이 열의 해당하는 NaN 값은 최대 빈도인 `mode`를 사용했다. `mode`를 사용한 이유는 아무래도
높은 등급에 타는 사람들보다 일반 등급에 기차를 타는 사람인 확률이 높으므로 사용했을 것 같다. 이처럼 categorical data의 imputation 은 모두 mode로 처리하였고, 유일한 numerical value인 price 만 `mean`을 사용했다.
```py
az.plot_kde(data['price'].values, rug=True)
plt.yticks([0], alpha=0);
```
![1]({{site.url}}/assets/images/2_1.png)

`az.plot_kde` 는 주어진 data에서의 observed 된 값들의 distribution을 보여준다. 여기서 확인할 수 있는 사실은, 기차푯값의 분포는 약간 right skewed distribution처럼 보이긴 하지만 만약 오른쪽 끝 부분을 자른다고 하면 Gaussian distribution처럼 보일 수 있다.
여기서 우리는 가정을 기찻값의 표의 분포는 **Gaussian distribution**을 따른다고 하자.

# Modelling
Bayesian estimation에서는 prior and likelihood 설정이 필요하다. 여기서는 prior 설정을 경험을 바탕으로 설정하였지만, prior 설정은 fix 된 값들이 아니며, 더 나은 정보들이 존재하면 그것을 사용하여 prior를 설정해도 상관없다. 그럼 무슨 prior를 설정해야 하는가? 앞에서 우리는 기찻값은 Gaussian distribution을 따른다고 했다. Gaussian distribution을 정의하기 위해서는 두 개의 파라미터 μ, σ가 필요하다. 따라서 우리가 μ, σ 값을 예측할 수 있게 되면 기찻값의 Population Distribution을 예측할 수 있게 된다.


  * Choices of priors:

    * μ를 기차푯값의 평균인데, 0보다 작을 수 없고, 경험적으로 300보다 비싸진 않은가보다 그래서 Uniform distribution을 (0, 300) 까지 설정한다.
    * σ를 기차푯값의 표준편차이기 때문에 당연히 0보다 크다고 알 수 있다. 따라서 half normal distribution 사용 (0보다 큰 쪽)
    

  * Choices for ticket price likelihood function:
    * y 위에서 정한 파라미터들을 통해서 관측된 데이터라고 하자, 그리고 기차푯값의 분포는 Normal distribution으로 정했다.
    * 1000 posterior samples 
>Normal distribution과 Gaussian distribution은 비슷하다. 따라서 기차푯값의 분포를 Normal distribution으로 설정했다.


```py
with pm.Model() as model_g:
    μ = pm.Uniform('μ', lower=0, upper=300)
    σ = pm.HalfNormal('σ', sd=10)
    y = pm.Normal('y', mu=μ, sd=σ, observed=data['price'].values)
    trace_g = pm.sample(1000, tune=1000)
```
>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Sequential sampling (2 chains in 1 job)
NUTS: [σ, μ]
Sampling chain 0, 0 divergences: 100%|██████████| 2000/2000 [00:06<00:00, 328.64it/s]
Sampling chain 1, 0 divergences: 100%|██████████| 2000/2000 [00:04<00:00, 445.79it/s]

앞의 우리가 가정한 사실들을 바탕으로 PyMC3를 사용하여 Model 설정하였다. 그리고 우리가 세운 모델로 1,000개의 표본 뽑기를 2번 했다는 결과도 확인할 수 있다. Model 설정에서 중요한 사실은 parameter 들마다 각각의 distribution을 갖고 있다는 것이다. 이것이 Bayesian estimation의 특징인데, 지금 우리가 가진 데이터로 어떠한 결론을 낼 수 있다고 하더라도, 나중에 다른 데이터를 받아들여서 만약 그 결과가 바뀌게 될 수도 있다는 사실을 내포하고 있다.

```py
az.plot_trace(trace_g);
```
![2]({{site.url}}/assets/images/2_2.png)


* 왼쪽은 2 chain of sample의 `kde` distribution을 각 parameter에 대해 보여주고 있고, 오른쪽은 각 표본 뽑기 마다 실질적인 parameter의 값을 step by step 보여주고 있다.

* 각각의 parameter는 비슷한 shape를 보여주고 크게 눈에 띄는 이상한 모양이나 값이 없음을 확인할 수 있다. 이 말은, 더 많은 표본 뽑기 chain을 만들게 되면 각 parameter의 가장 peak 인부분으로 converge 함을 알 수 예상 할 수 있다..

여기서 중요한 사실은 convergence이다. 앞서 prior 설정은 크게 상관없다는 이유를 여기서 알 수 있다. 출발을 어디서 하든, 만약 우리가 추론하려는 parameter가 converge 한다는 사실을 알면 결국 그 끝에는 같은 결과를 마주하게 되기 때문이다. 

```py
az.plot_joint(trace_g, kind='kde', fill_last=False);
```
![3]({{site.url}}/assets/images/2_3.png)

위의 estimate는 2개의 parameter 를 예측 하기 때문에, 2 dimension prediction 이고, 위의 그래프 처럼 joint graph 를 통해서 서로 다른 2개의 parameter 는 independent 함을 알 수 있다. 왜냐하면 각 각의 peak에서 서로의 best case 를 확인 할 수 있기 때문이다. 


```py
az.summary(trace_g)
```
 mean | sd | hpd_3% | hpd_97% | mcse_mean | mcse_sd | ess_mean | ess_sd | ess_bulk | ess_tail | r_hat
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
62.377 | 1.420 | 59.679 | 64.981 | 0.032 | 0.023 | 1925.0 | 1925.0 | 1928.0 | 1569.0 | 1.0
23.581 | 1.033 | 21.678 | 25.583 | 0.026 | 0.018 | 1630.0 | 1626.0 | 1641.0 | 1334.0 | 1.0

`az.summary` 는 간단한 descriptive statistics 를 확인 할 수 있다.

```py
az.plot_posterior(trace_g);
```
![4]({{site.url}}/assets/images/2_4.png)



여기서는 그래프의 해석을 그대로 가져와 해석한 것이다. 
* 앞에서 확인 할 수 있듯이, 각각의 parameter 에 대해서 분포가 있음을 확인 할 수 있다.
*  Highest Posterior Density (HPD) 구간이 94% 설정 되어 있음을 알 수 있다. 따라서 반복적으로 데이터를 추가하더라도, 94% 확률로 HPD 안에 있을 확률을 나타낸다.
 * Bayesian inference 를 바탕으로 population mean 은 94% 의 HPD 를 통해서 63.8 에서 64.4 population standard deviation 은 24.5 에서 24.9임을 추론 할 수 있다. 

```py
pm.gelman_rubin(trace_g)
```
> Test 값이 1에 가깝다는 말은 converge 한다는 말이고, 더 많은 posterior 샘플을 추가해도 결국은 우리가 앞에 찾은 inference value 와 크게 다를게 없다고 해석할 수 있다.

# Model Validation
마지막으로 우리가 추론한 값의 타당성을 얻기 위해서, 설정한 모델의 validation을 확인 해야 한다. 방법으로는, `pm_sample_posterior_predictive` 을 통해서 우리가 추론한 parameter 들을 바탕으로 posterior sample을 만들어 비교해볼 것이다.

```py
ppc = pm.sample_posterior_predictive(trace_g, samples=1000, model=model_g)
np.asarray(ppc['y']).shape
``` 
> /usr/local/lib/python3.6/dist-packages/pymc3/sampling.py:1247: UserWarning: samples parameter is smaller than nchains times ndraws, some draws and/or chains may not be represented in the returned posterior predictive sample
  "samples parameter is smaller than nchains times ndraws, some draws "
100%|██████████| 1000/1000 [00:03<00:00, 329.56it/s]
(1000, 25798)
완전 새로운 1000번의 sampling 을 통해서 25798의 표본 데이터를 얻었다. 

```py
_, ax = plt.subplots(figsize=(10, 5))
ax.hist([y.mean() for y in ppc['y']], bins=19, alpha=0.5)
ax.axvline(data.price.mean())
ax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');
```

![5]({{site.url}}/assets/images/5_5.png)

그래프를 보면 실제 우리의 데이터의 기찻값의 평균과 예측된 parameter 들로 sampling 한 결과가 거의 일치 한다고 볼 수 있다. 따라서 모델의 타당성까지 확인 할 수 있다.

# 결과
우리는 Spanish High Speed Rail tickets pricing data set을 통하여, 기찻값의 population mean과 population standard deviation을 예측하여 기찻값 표에 대한 population distribution을 만들 수 있었다. 기찻값의 표는 Gaussian distribution을 따르며, 평균은 64의 근접 할 것이며, 표준편차는 약 24 정도 될 것이다. 지금까지 Bayesian estimation 과정을 따라오면서 느낄 수 있었던, Bayesian estimation의 특징은 다음과 같이 볼 수 있다.
* 불확실성에 대해서 열려있다.
* 모든 예측 파라미터에 대해서 각각의 분포가 존재함 (모든 예측된 값은 변할 가능성을 암시)

아무리 우리가 예측한 값으로 converge 한다는 사실을 알아도, 100% 확신을 할 수는 없다. 어디까지나 모든 통계의 테스트와 같이 통계적 유의미를 갖는 결과일 뿐, 절대적인 진리나 사실이 아님을 다시 한 번 상기하면서 이 글을 마친다.

(Reference : https://towardsdatascience.com/hands-on-bayesian-statistics-with-python-pymc3-arviz-499db9a59501 )